{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Report — Complete Operation 1 + Report Generation\n",
    "\n",
    "Complete Operation 1 from Flash_Report.py with Excel report generation capabilities.\n",
    "\n",
    "**Instructions:**\n",
    "1. Update file paths in the configuration cell\n",
    "2. Run all cells in order\n",
    "3. Check output Excel files in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from difflib import get_close_matches\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration - Update File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Update these paths to your actual files\n",
    "file_path = r'REPLACE_WITH_MAIN_DATA_FILE.xlsx'  # Web Intelligence Export\n",
    "source_path_ca = r'REPLACE_WITH_CA_REFERENCE_FILE.xlsx'  # CA reference (Sheet1)\n",
    "source_path_us = r'REPLACE_WITH_US_REFERENCE_FILE.xlsx'  # US reference (Sheet1)\n",
    "\n",
    "# Output configuration\n",
    "output_us_file = 'Final_Report_USD_US.xlsx'\n",
    "output_ca_file = 'Final_Report_Canada_CA.xlsx'\n",
    "us_partners_folder = 'US_partners_report'\n",
    "ca_partners_folder = 'Canada_partners_report'\n",
    "\n",
    "print('Configuration:')\n",
    "print('Main:', file_path)\n",
    "print('CA Ref:', source_path_ca)\n",
    "print('US Ref:', source_path_us)\n",
    "print('Output US:', output_us_file)\n",
    "print('Output CA:', output_ca_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_mappings():\n",
    "    return {\n",
    "        'main_file_mappings': {\n",
    "            'SRC_SYS_KY': ['SRC_SYS_KY', 'Src Sys Ky'],\n",
    "            'CROSS_SOURCED': ['CROSS_SOURCED', 'Cross Sourced'],\n",
    "            'BDE_FLAG': ['BDE_FLAG', 'Bde Flag'],\n",
    "            'MSP_FLAG': ['MSP_FLAG', 'MSP Flag'],\n",
    "            'REPORTING_TYPE': ['REPORTING_TYPE', 'Reporting Type'],\n",
    "            'PRODUCT_LINE': ['PRODUCT_LINE', 'Product Line'],\n",
    "            'RESELLER_PARTY_ID': ['RESELLER_PARTY_ID', 'Reseller Party Id'],\n",
    "            'DISTRIBUTOR_PARTY_ID': ['DISTRIBUTOR_PARTY_ID', 'Distributor Party Id'],\n",
    "            'FISCAL_MONTH': ['FISCAL_MONTH', 'Fiscal Month'],\n",
    "            'NDP_TOTAL_USD': ['NDP_TOTAL_USD', 'Ndp Total Usd'],\n",
    "            'NET_TOTAL_USD': ['NET_TOTAL_USD', 'Net Total Usd'],\n",
    "            'UPFRONT_DISCOUNT_AMT_USD': ['UPFRONT_DISCOUNT_AMT_USD', 'Upfront Discount Amt Usd'],\n",
    "            'BACKEND_DISCOUNT_AMT_USD': ['BACKEND_DISCOUNT_AMT_USD', 'Backend Discount Amt Usd'],\n",
    "            'DATA_TYPE': ['DATA_TYPE', 'Data Type'],\n",
    "            'BACKEND_DEAL_1': ['BACKEND_DEAL_1', 'Backend Deal 1'],\n",
    "            'INVOICE_NUMBER': ['INVOICE_NUMBER', 'Invoice Number'],\n",
    "            'HPE_SALES_ORDER_NUMBER': ['HPE_SALES_ORDER_NUMBER', 'Hpe Sales Order Number'],\n",
    "            'NET_TOTAL_LC': ['NET_TOTAL_LC', 'Net Total Lc'],\n",
    "            'BACKEND_DISCOUNT_AMT_LC': ['BACKEND_DISCOUNT_AMT_LC', 'Backend Discount Amt Lc'],\n",
    "            'UPFRONT_DISCOUNT_AMT_LC': ['UPFRONT_DISCOUNT_AMT_LC', 'Upfront Discount Amt Lc'],\n",
    "            'NDP_TOTAL_LC': ['NDP_TOTAL_LC', 'Ndp Total Lc'],\n",
    "            'DISTRIBUTOR_PARTY_NAME': ['Distributor Party Name', 'DISTRIBUTOR_PARTY_NAME'],\n",
    "            'RESELLER_PARTY_NAME': ['Reseller Party Name', 'RESELLER_PARTY_NAME'],\n",
    "            'PRODUCT_NUMBER': ['Product Number', 'PRODUCT_NUMBER']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def load_column_mappings():\n",
    "    try:\n",
    "        with open('column_mappings.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print('[WARNING] column_mappings.json not found, using default mappings')\n",
    "        return get_default_mappings()\n",
    "    except json.JSONDecodeError:\n",
    "        print('[ERROR] Invalid JSON in column_mappings.json, using default mappings')\n",
    "        return get_default_mappings()\n",
    "\n",
    "def find_column_match(target_column, available_columns, mappings):\n",
    "    if target_column in mappings:\n",
    "        for variant in mappings[target_column]:\n",
    "            if variant in available_columns:\n",
    "                return variant\n",
    "        for variant in mappings[target_column]:\n",
    "            close_matches = get_close_matches(variant, available_columns, n=1, cutoff=0.8)\n",
    "            if close_matches:\n",
    "                return close_matches[0]\n",
    "    close_matches = get_close_matches(target_column, available_columns, n=1, cutoff=0.7)\n",
    "    if close_matches:\n",
    "        return close_matches[0]\n",
    "    return None\n",
    "\n",
    "def standardize_column_names(df, file_type='main'):\n",
    "    mappings = load_column_mappings()\n",
    "    if file_type == 'main':\n",
    "        target_mappings = mappings.get('main_file_mappings', {})\n",
    "    else:\n",
    "        target_mappings = mappings.get('reference_file_mappings', {})\n",
    "    column_mapping = {}\n",
    "    available_columns = list(df.columns)\n",
    "    print(f\"[*] Standardizing {file_type} file columns...\")\n",
    "    for target_col, variants in target_mappings.items():\n",
    "        matched_col = find_column_match(target_col, available_columns, target_mappings)\n",
    "        if matched_col:\n",
    "            column_mapping[matched_col] = target_col\n",
    "            print(f\"[*] Mapped '{matched_col}' -> '{target_col}'\")\n",
    "        else:\n",
    "            print(f\"[WARNING] No match found for required column: {target_col}\")\n",
    "    df_standardized = df.rename(columns=column_mapping)\n",
    "    return df_standardized, column_mapping\n",
    "\n",
    "def validate_main_file(df):\n",
    "    df_standardized, column_mapping = standardize_column_names(df, 'main')\n",
    "    required_columns = ['SRC_SYS_KY','CROSS_SOURCED','BDE_FLAG','MSP_FLAG','REPORTING_TYPE','PRODUCT_LINE','RESELLER_PARTY_ID','DISTRIBUTOR_PARTY_ID','FISCAL_MONTH','NDP_TOTAL_USD','NET_TOTAL_USD','UPFRONT_DISCOUNT_AMT_USD','BACKEND_DISCOUNT_AMT_USD','DATA_TYPE','BACKEND_DEAL_1','INVOICE_NUMBER','HPE_SALES_ORDER_NUMBER','NET_TOTAL_LC','BACKEND_DISCOUNT_AMT_LC','UPFRONT_DISCOUNT_AMT_LC','NDP_TOTAL_LC']\n",
    "    missing_columns = [col for col in required_columns if col not in df_standardized.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Main file missing columns: {missing_columns}\")\n",
    "    print('[*] Main file format validation passed')\n",
    "    return df_standardized\n",
    "\n",
    "def validate_reference_file(df, file_type):\n",
    "    df_standardized, column_mapping = standardize_column_names(df, 'reference')\n",
    "    required_columns = ['PL','BU','TYPE','EXCLUSION_PARTY_ID','EXCLUSION_LEVEL','PG_EXCLUSION_ELIGIBLE_LIST_PARTY_ID','LOC_ID','ELICPES','PN_PL','BU_1','COMMON_PL','COMMON_PN_PL']\n",
    "    missing_columns = [col for col in required_columns if col not in df_standardized.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"{file_type} reference file missing columns: {missing_columns}\")\n",
    "    print(f\"[*] {file_type} reference file format validation passed\")\n",
    "    return df_standardized\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Validate Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[*] Reading main data file...')\n",
    "df_raw = pd.read_excel(file_path, engine='openpyxl')\n",
    "df = validate_main_file(df_raw)\n",
    "print(f\"[*] Main data file successfully loaded. Total rows: {len(df)}\")\n",
    "\n",
    "print('[*] Reading the Reference File for CA (Sheet1) ...')\n",
    "df_source_ca_raw = pd.read_excel(source_path_ca, sheet_name='Sheet1')\n",
    "df_source_ca = validate_reference_file(df_source_ca_raw, 'CA')\n",
    "\n",
    "print('[*] Reading the Reference File for US (Sheet1) ...')\n",
    "df_source_us_raw = pd.read_excel(source_path_us, sheet_name='Sheet1')\n",
    "df_source_us = validate_reference_file(df_source_us_raw, 'US')\n",
    "\n",
    "print('All input files loaded and validated successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation 1 — Data Filtering and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('OPERATION 1: BOE Data Validation and Filtering')\n",
    "print('='*60)\n",
    "\n",
    "# Filter for SRC_SYS_KY in [2032, 2866]\n",
    "df_src = df[df['SRC_SYS_KY'].isin([2032, 2866])]\n",
    "print(f\"[*] After filtering for SRC_SYS_KY: {len(df_src)} rows\")\n",
    "\n",
    "# Remove CROSS_SOURCED == 'Y'\n",
    "df_cross_sourced = df_src[df_src['CROSS_SOURCED'] != 'Y']\n",
    "print(f\"[*] After removing CROSS_SOURCED Y: {len(df_cross_sourced)} rows\")\n",
    "\n",
    "# Remove BDE_FLAG == 'Y' and fill N\n",
    "df_bde = df_cross_sourced[df_cross_sourced['BDE_FLAG'] != 'Y'].copy()\n",
    "df_bde['BDE_FLAG'] = df_bde['BDE_FLAG'].fillna('N')\n",
    "print(f\"[*] After removing BDE_FLAG Y: {len(df_bde)} rows\")\n",
    "\n",
    "# Remove MSP_FLAG == 'T'\n",
    "df_msp = df_bde[df_bde['MSP_FLAG'] != 'T']\n",
    "print(f\"[*] After removing MSP_FLAG T: {len(df_msp)} rows\")\n",
    "\n",
    "# Remove REPORTING_TYPE == 'RCS' (also keep RCS separately for reference)\n",
    "df_reporting = df_msp[df_msp['REPORTING_TYPE'] != 'RCS']\n",
    "df_rcs = df_msp[df_msp['REPORTING_TYPE'] == 'RCS']\n",
    "print(f\"[*] After removing REPORTING_TYPE RCS: {len(df_reporting)} rows\")\n",
    "print(f\"[*] RCS Data for reference: {len(df_rcs)} rows\")\n",
    "\n",
    "# Add BU/BU_Type/Scheme_Name and reset index\n",
    "df_extend_columns = df_reporting.assign(BU='', BU_Type='', Scheme_Name='').reset_index(drop=True)\n",
    "print(f\"[*] After adding columns and resetting index: {len(df_extend_columns)} rows\")\n",
    "\n",
    "print('\\nData filtering completed successfully!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
